from typing import Optional, List, Any, Dict

import attr
import numpy as np
import scipy.sparse as sp

from conferences.cikm.cfgan.our_implementation.constants import CFGANMode, CFGANMaskType
from conferences.cikm.cfgan.our_implementation.models.v1_compat.CFGANModel import CFGANModel
from conferences.cikm.cfgan.our_implementation.parameters import CFGANHyperParameters
from recsys_framework.Recommenders.BaseRecommender import BaseRecommender
from recsys_framework.Recommenders.DataIO import DataIO
from recsys_framework.Recommenders.Incremental_Training_Early_Stopping import Incremental_Training_Early_Stopping
from recsys_framework.Utils.conf_logging import get_logger

logger = get_logger(__name__)


class CFGANRecommenderEarlyStopping(
    BaseRecommender,  # type: ignore
    Incremental_Training_Early_Stopping  # type: ignore
):
    """A TopK recommender backed by a CFGAN.

    Because CFGAN is a conditioned GAN, we need the condition vector to generate the item
    weights. In the CFGAN case, the condition vector is the user's interactions.

    In CFGAN, the generator creates *item weights* instead of interactions (ratings or
    implicit). Based on that, the item scores are simply the weights generated by the
    generator after we pass the condition vector.
    """

    RECOMMENDER_NAME = "CFGANRecommenderEarlyStopping"
    MAX_NUM_EPOCHS = 400
    MIN_NUM_EPOCHS = 200

    EARLY_STOPPING_KWARGS_KEYS = {
        "epochs_max",
        "epochs_min",
        "validation_every_n",
        "stop_on_validation",
        "validation_metric",
        "lower_validations_allowed",
        "evaluator_object",
        "algorithm_name",
    }

    def __init__(
        self,
        urm_train: sp.csr_matrix,
        num_training_item_weights_to_save: int = 0,
    ):
        super().__init__(URM_train=urm_train)

        self.model: Optional[CFGANModel] = None
        self.hyper_parameters: Optional[CFGANHyperParameters] = None
        self.early_stopping_kwargs: Optional[Dict[str, Any]] = None
        self.item_weights: Optional[np.ndarray] = None
        self.num_training_item_weights_to_save = num_training_item_weights_to_save

    @staticmethod
    def get_recommender_name(
        cfgan_mode: CFGANMode,
        cfgan_mask_type: CFGANMaskType,
    ) -> str:
        return (
            f"{CFGANRecommenderEarlyStopping.RECOMMENDER_NAME}_"
            f"{cfgan_mode.value}_"
            f"{cfgan_mask_type.value}"
        )

    def save_model(
        self,
        folder_path: str,
        file_name: Optional[str] = None
    ) -> None:
        if file_name is None:
            file_name = self.RECOMMENDER_NAME

        if self.model is not None:
            self.model.save_model(
                folder_path=folder_path,
                file_name=file_name
            )

        data_io = DataIO(
            folder_path=folder_path
        )
        data_io.save_data(
            file_name=file_name,
            data_dict_to_save={
                "item_weights": self.item_weights,
                "hyper_parameters": attr.asdict(self.hyper_parameters),
                "early_stopping_kwargs": self.early_stopping_kwargs,
                "num_training_item_weights_to_save": self.num_training_item_weights_to_save,
            }
        )

        logger.info(f"Saved Attributes at '{folder_path}{file_name}_metadata.zip'.")

    def load_model(
        self,
        folder_path: str,
        file_name: Optional[str] = None,
    ) -> None:
        if file_name is None:
            file_name = self.RECOMMENDER_NAME

        data_io = DataIO(
            folder_path=folder_path
        )
        data_dict = data_io.load_data(
            file_name=file_name
        )

        self.hyper_parameters = CFGANHyperParameters(
            **data_dict["hyper_parameters"]
        )
        self.early_stopping_kwargs = data_dict["early_stopping_kwargs"]
        self.item_weights = data_dict["item_weights"]
        self.num_training_item_weights_to_save = data_dict.get(
            "num_training_item_weights_to_save",
            0,
        )

        self.model = CFGANModel(
            urm_train=self.URM_train,
            hyper_parameters=self.hyper_parameters,
            num_training_item_weights_to_save=self.num_training_item_weights_to_save,
            initialize_model=False,
        )
        self.model.load_model(
            folder_path=folder_path,
            file_name=file_name
        )

    def _parse_fit_kwargs(
        self,
        **kwargs: dict[str, Any]
    ) -> tuple[dict[str, Any], dict[str, Any]]:
        hyper_parameters_kwargs: dict[str, Any] = dict()
        early_stopping_kwargs: dict[str, Any] = dict()

        for k, v in kwargs.items():
            if k in self.EARLY_STOPPING_KWARGS_KEYS:
                early_stopping_kwargs[k] = v
            else:
                hyper_parameters_kwargs[k] = v

        # When doing early-stopping and hyper-parameter tuning, the number of epochs is not sent as hyper-parameter.
        # This means that inside the if clause we're dealing with early-stopping.
        if "epochs" not in hyper_parameters_kwargs:
            # This class does not uses the 'epochs' attribute of the CFGANHyperParameters class.
            # However, other classes such as CFGANRecommender and GuidelineCFGANRecommender need to know
            # the number of epochs. Moreover, this attribute 'epochs' will be set at the end of the
            # early-stopping process with the epoch in which we obtained the best recommendation metric.
            hyper_parameters_kwargs["epochs"] = self.MAX_NUM_EPOCHS

            # Set-up early-stopping to go at least to half the maximum number of the epochs.
            # The final training, when evaluating on test set, will use the optimal number of epochs X,
            # regardless if X is lower than the minimum.
            # Setting a minimum when early-stopping ensures that we explore more even if we already found a
            # "good-enough" validation result.
            if "epochs_max" not in early_stopping_kwargs:
                early_stopping_kwargs["epochs_max"] = self.MAX_NUM_EPOCHS

            if "epochs_min" not in early_stopping_kwargs:
                early_stopping_kwargs["epochs_min"] = self.MIN_NUM_EPOCHS

        # When doing the evaluation on the test set, the 'epochs' hyper-parameter is sent inside
        # 'hyper_parameters_kwargs'. Entering the else clause means we're dealing with the evaluation on the test set.
        else:
            # We need to set 'epochs_max' in 'early_stopping_kwargs' because this is the way to tell the recommender
            # how many epochs we're doing in this training, even though we will not be doing early-stopping.
            early_stopping_kwargs["epochs_max"] = hyper_parameters_kwargs["epochs"]

        return hyper_parameters_kwargs, early_stopping_kwargs

    def fit(
        self,
        **kwargs: dict[str, Any]
    ) -> None:
        hyper_parameters_kwargs, early_stopping_kwargs = self._parse_fit_kwargs(
            **kwargs
        )

        try:
            self.hyper_parameters = CFGANHyperParameters(**hyper_parameters_kwargs)
        except TypeError as e:
            logger.exception(
                f"Kwargs {kwargs} could not be converted into CFGANHyperParameters"
            )
            raise e

        self.RECOMMENDER_NAME = self.get_recommender_name(
            cfgan_mode=self.hyper_parameters.mode,
            cfgan_mask_type=self.hyper_parameters.mask_type,
        )
        early_stopping_kwargs["algorithm_name"] = self.RECOMMENDER_NAME

        self.model = CFGANModel(
            urm_train=self.URM_train,
            hyper_parameters=self.hyper_parameters,
            num_training_item_weights_to_save=self.num_training_item_weights_to_save,
            initialize_model=True,
        )

        self._prepare_model_for_validation()
        self._update_best_model()
        self._train_with_early_stopping(
            **early_stopping_kwargs,
        )
        self._prepare_model_for_validation()

    def _run_epoch(
        self,
        num_epoch: int
    ) -> None:
        if self.model is None:
            raise ValueError("Model is None")

        self.model.run_epoch(
            epoch=num_epoch,
        )

    def _prepare_model_for_validation(
        self
    ) -> None:
        if self.model is None:
            raise ValueError("Model is None")

        self.item_weights = self.model.get_item_weights(
            urm=None,
        )
        logger.info(f"Prepared {self.RECOMMENDER_NAME} for validation")

    def _update_best_model(
        self
    ) -> None:
        if self.model is None:
            raise ValueError("Model is None")

        self.item_weights = self.model.get_item_weights(
            urm=None,
        )
        logger.info(f"Updated best model for {self.RECOMMENDER_NAME}")

    def _compute_item_score(
        self,
        user_id_array: np.ndarray,
        items_to_compute: Optional[List[int]] = None
    ) -> np.ndarray:
        if self.item_weights is None:
            raise ValueError("Model has not been fitted, thus cannot recommend.")

        num_users = user_id_array.shape[0]

        if items_to_compute is None:
            return self.item_weights[user_id_array, :]

        item_scores = -np.ones((num_users, self.n_items), dtype=np.float32) * np.inf
        item_scores[:, items_to_compute] = self.item_weights[user_id_array, items_to_compute]
        return item_scores
